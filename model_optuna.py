import os
import json
import deepxde as dde
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
import argparse
import shutil
import torch.nn as nn
import optuna


def create_optuna_objective(args, log_dir):
    """åˆ›å»ºOptunaä¼˜åŒ–ç›®æ ‡å‡½æ•°"""

    def objective(trial):
        #data
        # branch_file=trial.suggest_categorical('branch_file', ["branch_net_K_sampled.csv","herstory_branch_net_K_sampled.csv"])
        branch_file="herstory_branch_net_K_sampled.csv"
        # isPCA=trial.suggest_categorical('isPCA', [True,False])
        isPCA=False
        # pca_dim = trial.suggest_int('pca_dim', 64, 3000, step=32)
        pca_dim=512
        # scaling_method=trial.suggest_categorical('scaling_method',['standard','minmax','none'])

        # model
        p = trial.suggest_int('output_dim', 32, 512, step=32)  # ä¾‹å¦‚ä»64åˆ°512ï¼Œæ­¥é•¿ä¸º32 # 0. é¦–å…ˆå»ºè®®ä¸€ä¸ªå…±äº«çš„è¾“å‡ºç»´åº¦ p,è¿™ä¸ªpå°†æ˜¯branch netå’Œtrunk netæœ€åä¸€å±‚çš„ç¥ç»å…ƒæ•°é‡

        # å®šä¹‰åˆ†æ”¯ç½‘ç»œï¼ˆBranch Netï¼‰çš„è¶…å‚æ•°
        branch_num_layers = trial.suggest_int('branch_num_layers', 2, 5)
        branch_layers = []
        for i in range(branch_num_layers - 1):  # æ³¨æ„ï¼šæœ€åä¸€å±‚æˆ‘ä»¬å›ºå®šä¸ºpï¼Œæ‰€ä»¥è¿™é‡Œå°‘ä¸€å±‚
            # å»ºè®®æ¯ä¸€å±‚çš„ç¥ç»å…ƒæ•°é‡ï¼Œä½†æœ€åä¸€å±‚å°šæœªç¡®å®š
            hidden_units = trial.suggest_int(f'branch_layer_{i}', 32, 512, step=32)
            branch_layers.append(hidden_units)
        # åˆ†æ”¯ç½‘ç»œçš„æœ€åä¸€å±‚å°±æ˜¯è¾“å‡ºå±‚ï¼Œå…¶å¤§å°å¿…é¡»ä¸º p
        branch_layers.append(p)  # è¿™æ˜¯å…³é”®çº¦æŸï¼

        # å®šä¹‰ä¸»å¹²ç½‘ç»œï¼ˆTrunk Netï¼‰çš„è¶…å‚æ•°
        trunk_num_layers = trial.suggest_int('trunk_num_layers', 2, 5)
        trunk_layers = []
        for i in range(trunk_num_layers - 1):
            hidden_units = trial.suggest_int(f'trunk_layer_{i}', 32, 256, step=32)
            trunk_layers.append(hidden_units)
        # ä¸»å¹²ç½‘ç»œçš„æœ€åä¸€å±‚ä¹Ÿå¿…é¡»æ˜¯ p
        trunk_layers.append(p)  # è¿™æ˜¯å…³é”®çº¦æŸï¼

        # model'
        activation = trial.suggest_categorical('activation',
                                               ['tanh', 'relu',  'sigmoid', 'gelu',  'elu'])
        initializer = trial.suggest_categorical('initializer',
                                                ['Glorot normal', 'Glorot uniform', 'He normal', 'He uniform'])
        # is_output_activation = trial.suggest_categorical('is_output_activation', [True, False])
        is_output_activation=args.is_output_activation
        output_activation = trial.suggest_categorical('output_activation', ['relu', 'sigmoid', 'relu6', 'softplus'])
        isDropout = trial.suggest_categorical('isDropout', [True, False])
        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.3, step=0.1)
        is_bias=trial.suggest_categorical('is_bias', [True,False])

        # training
        epochs=trial.suggest_int('epochs', 10000,50000,step=10000)
        optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'adamw', 'rmsprop'])
        # optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'adamw', 'rmsprop','L-BFGS','NNCG','L-BFGS-B'])
        lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True) # 1. å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´ [2,3](@ref)
        weight_decay = trial.suggest_float('weight_decay', 1e-8, 1e-4, log=True)
        decay_name_list=['exponential', 'cosine', 'step','inverse time']
        decay_name= trial.suggest_categorical('decay_name',decay_name_list)

        if decay_name == "exponential":
            gamma=trial.suggest_float('gamma',0.9,0.999,step=0.09)
            decay=("exponential",gamma)
        elif decay_name == "cosine":
            T_max=trial.suggest_float('T_max',lr * 1e-3,lr * 1e-2,step=lr * 2e-3)
            eta_min=trial.suggest_int('eta_min',epochs/10,epochs/2,step=epochs/10)
            decay=("cosine", T_max, eta_min)
        elif decay_name == "step":
            step_size=trial.suggest_int('step_size',epochs/2,epochs,step=epochs/10)
            gamma=trial.suggest_float('gamma',0.5,0.9,step=0.1)
            decay =("step", step_size, gamma)
        elif decay_name == "inverse time":
            decay_steps=trial.suggest_int('decay_steps',epochs/2,epochs,step=epochs/10)
            decay_rate=trial.suggest_float('decay_rate',0.001,0.1,step=0.01)
            decay=("inverse time", decay_steps, decay_rate)
        else:
            decay=None

        # early_stopping
        isES= trial.suggest_categorical('isES',[True,False])
        min_delta = trial.suggest_float('min_delta', 1e-8, 1e-6, step=1e-7)
        patience = trial.suggest_int('patience', 2000, 10000, step=2000)
        baseline = trial.suggest_float('baseline', 1e-6, 1e-5, step=2e-6)
        start_from_epoch = trial.suggest_int('start_from_epoch', 2000, 6000, step=1000)

        # åŠ è½½æ•°æ®
        branch_df, trunk_df, temp_df, temp_df_with_titles = load_data(
            branch_file,
            args.trunk_file,
            args.temp_file
        )

        # å‡†å¤‡æ•°æ®
        data, X_branch, scale_params, X_branch_test_scaled, X_trunk_test, y_test = prepare_data(
            branch_df,
            trunk_df,
            temp_df,
            isPCA=isPCA,
            pca_dim=pca_dim,
            test_size=args.test_size,
            random_state=args.random_state,
            scaling_method=args.scaling_method
        )
        print("ğŸ¯ ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–")


        # 2. åˆ›å»ºæ¨¡å‹ [5,7](@ref)
        model = create_model(
            data,
            branch_dim=X_branch.shape[1],
            trunk_dim=3,
            branch_layers=branch_layers,
            trunk_layers=trunk_layers,
            activation=activation,
            initializer=initializer,
            is_output_activation=is_output_activation,
            output_activation=output_activation,
            isDropout=isDropout,
            dropout_rate=dropout_rate,
            is_bias=is_bias,
            init_bias=scale_params.get("y_mean", 0.0)
        )

        # 3. é…ç½®ä¼˜åŒ–å™¨
        if optimizer_name == "adamw":
            optimizer = torch.optim.AdamW(
                model.net.parameters(),
                lr=lr,
                weight_decay=weight_decay
            )
            model.compile(optimizer, lr=lr, metrics=args.metrics)
        else:
            model.compile(optimizer_name, lr=lr,decay=decay, metrics=args.metrics)

        # 4. åˆ›å»ºå›è°ƒ - ä¸ºæ¯ä¸ªtrialåˆ›å»ºç‹¬ç«‹çš„æ—¥å¿—ç›®å½•
        trial_log_dir = os.path.join(log_dir, f"trial_{trial.number}")
        trial_writer = SummaryWriter(log_dir=trial_log_dir)

        early_stopping = dde.callbacks.EarlyStopping(
            monitor=args.monitor,
            min_delta=min_delta,
            patience=patience,
            baseline=baseline,
            start_from_epoch=start_from_epoch
        )

        # 5. è®­ç»ƒæ¨¡å‹ï¼ˆå‡å°‘epochsä»¥åŠ é€Ÿæœç´¢ï¼‰
        epochs_per_trial = min(args.epochs, 1000)  # é™åˆ¶æ¯ä¸ªtrialçš„è®­ç»ƒè½®æ¬¡

        try:
            if isES:
                losshistory, train_state = model.train(
                    iterations=epochs_per_trial,
                    display_every=args.display_every * 10,  # å‡å°‘è¾“å‡ºé¢‘ç‡
                    callbacks=[early_stopping],
                    verbose=0  # å‡å°‘è¾“å‡º
                )
            else:
                losshistory, train_state = model.train(
                    iterations=epochs_per_trial,
                    display_every=args.display_every * 10,  # å‡å°‘è¾“å‡ºé¢‘ç‡
                    verbose=0  # å‡å°‘è¾“å‡º
                )

            # 6. è¿”å›éªŒè¯æŸå¤±ä½œä¸ºä¼˜åŒ–ç›®æ ‡ [1,2](@ref)
            best_test_loss = min([sum(loss) for loss in losshistory.loss_test])
            return best_test_loss

        except Exception as e:
            print(f"Trial {trial.number} failed: {e}")
            return float('inf')  # è¿”å›ä¸€ä¸ªå¾ˆå¤§çš„å€¼è¡¨ç¤ºå¤±è´¥

    return objective


def run_optuna_optimization(args, log_dir):
    """è¿è¡ŒOptunaè¶…å‚æ•°ä¼˜åŒ–"""

    # åˆ›å»ºOptunaç ”ç©¶ [2,3](@ref)
    study = optuna.create_study(
        direction='minimize',  # æœ€å°åŒ–éªŒè¯æŸå¤±
        sampler=optuna.samplers.TPESampler(seed=args.random_state),
        pruner=optuna.pruners.MedianPruner()  # æå‰å‰ªæ
    )

    # åˆ›å»ºç›®æ ‡å‡½æ•°
    objective = create_optuna_objective(
        args,log_dir
    )

    # è¿è¡Œä¼˜åŒ– [1](@ref)
    print("å¼€å§‹Optunaè¶…å‚æ•°ä¼˜åŒ–...")
    study.optimize(
        objective,
        n_trials=200,  # è¯•éªŒæ¬¡æ•°
        timeout=36000,  # æœ€å¤§è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        show_progress_bar=True
    )

    return study


def analyze_optuna_results(study, output_dir):
    """åˆ†æå’Œå¯è§†åŒ–Optunaç»“æœ"""

    print("\n" + "=" * 60)
    print("Optunaè¶…å‚æ•°ä¼˜åŒ–ç»“æœ")
    print("=" * 60)

    # è¾“å‡ºæœ€ä½³è¶…å‚æ•° [2](@ref)
    print(f"æœ€ä½³è¯•éªŒ: #{study.best_trial.number}")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {study.best_value:.6f}")
    print("æœ€ä½³è¶…å‚æ•°:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")

    # ä¿å­˜ç»“æœ
    results = {
        'best_trial_number': study.best_trial.number,
        'best_value': study.best_value,
        'best_params': study.best_params,
        'trials_dataframe': study.trials_dataframe().to_dict('records')
    }

    results_path = os.path.join(output_dir, "optuna_results.json")
    with open(results_path, "w") as f:
        json.dump(results, f, indent=4, default=str)  # æ·»åŠ default=str

    # å¯è§†åŒ–ç»“æœ [2,5](@ref)
    try:
        import optuna.visualization as vis

        # ä¼˜åŒ–å†å²
        fig_history = vis.plot_optimization_history(study)
        fig_history.write_image(os.path.join(output_dir, "optuna_optimization_history.png"))

        # è¶…å‚æ•°é‡è¦æ€§
        fig_importance = vis.plot_param_importances(study)
        fig_importance.write_image(os.path.join(output_dir, "optuna_param_importance.png"))

        # å¹³è¡Œåæ ‡å›¾
        fig_parallel = vis.plot_parallel_coordinate(study)
        fig_parallel.write_image(os.path.join(output_dir, "optuna_parallel_coordinate.png"))

        print(f"å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

    except Exception as e:
        print(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")

    return study.best_params

dde.backend.set_default_backend("pytorch")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ===========================================================
# å®éªŒé…ç½®å’Œç›®å½•ç®¡ç†
# ===========================================================

def create_experiment_dir(base_dir="runs", exp_name=None):
    """åˆ›å»ºå®éªŒç›®å½•ç»“æ„

    Args:
        base_dir: åŸºç¡€ç›®å½•
        exp_name: å®éªŒåç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ

    Returns:
        exp_dir: å®éªŒç›®å½•è·¯å¾„
        log_dir: æ—¥å¿—ç›®å½•è·¯å¾„
        ckpt_dir: æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    """
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M")

    # å¦‚æœæ²¡æœ‰æä¾›å®éªŒåç§°ï¼Œåˆ™ç›´æ¥ä½¿ç”¨æ—¶é—´æˆ³
    # å¦‚æœæä¾›äº†å®éªŒåç§°ï¼Œåˆ™ä½¿ç”¨åç§°+æ—¶é—´æˆ³
    if exp_name is None:
        exp_name = timestamp
    else:
        exp_name = f"{timestamp}_{exp_name}"

    # åˆ›å»ºå®éªŒç›®å½•ç»“æ„
    exp_dir = os.path.join(base_dir, exp_name)
    log_dir = os.path.join(exp_dir, "logs")
    ckpt_dir = os.path.join(exp_dir, "checkpoints")
    output_dir = os.path.join(exp_dir, "outputs")

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(exp_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(output_dir, exist_ok=True)

    print(f"åˆ›å»ºå®éªŒç›®å½•: {exp_dir}")
    return exp_dir, log_dir, ckpt_dir, output_dir


def save_config(config, exp_dir):
    """ä¿å­˜å½“å‰å®éªŒçš„é…ç½®åˆ° config.json"""
    config_path = os.path.join(exp_dir, "config.json")
    with open(config_path, "w") as f:
        json.dump(config, f, indent=4)
    print(f"é…ç½®å·²ä¿å­˜åˆ°: {config_path}")

    # åŒæ—¶ä¿å­˜ä¸€ä»½ä»£ç å¤‡ä»½
    src_file = os.path.abspath(__file__)
    dst_file = os.path.join(exp_dir, os.path.basename(__file__))
    shutil.copy2(src_file, dst_file)
    print(f"ä»£ç å·²å¤‡ä»½åˆ°: {dst_file}")


def save_checkpoint(model, optimizer, epoch, ckpt_dir, is_best=False):
    """ä¿å­˜æ¨¡å‹ checkpoint

    Args:
        model: æ¨¡å‹
        optimizer: ä¼˜åŒ–å™¨
        epoch: å½“å‰è½®æ¬¡
        ckpt_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
        is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
    """
    ckpt_path = os.path.join(ckpt_dir, f"epoch_{epoch:03d}.pt")
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
    }, ckpt_path)

    # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œåˆ™é¢å¤–ä¿å­˜ä¸€ä»½
    if is_best:
        best_path = os.path.join(ckpt_dir, "best_model.pt")
        shutil.copy2(ckpt_path, best_path)


# ===========================================================
# è‡ªå®šä¹‰TensorBoardå›è°ƒå‡½æ•°
# ===========================================================
class TensorBoardCallback(dde.callbacks.Callback):
    def __init__(self, writer, log_freq=1, ckpt_dir=None, save_freq=1, scale_params=None):
        """
        å‚æ•°:
            writer: SummaryWriterå®ä¾‹
            log_freq: è®°å½•é¢‘ç‡ï¼ˆæ¯å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡ï¼‰
            ckpt_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
            save_freq: ä¿å­˜æ£€æŸ¥ç‚¹é¢‘ç‡
            scale_params: ç”¨äºåç¼©æ”¾çš„å‚æ•°å­—å…¸

        """
        super().__init__()
        self.writer = writer
        self.log_freq = log_freq
        self.ckpt_dir = ckpt_dir
        self.save_freq = save_freq
        self.scale_params = scale_params  # <<< æ–°å¢
        self.step = 0
        self.best_loss = float('inf')

    def on_train_begin(self):
        """è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨"""
        print("è®­ç»ƒå¼€å§‹ï¼ŒTensorBoardè®°å½•å·²å¯ç”¨")

        # è®°å½•æ¨¡å‹ç»“æ„å›¾
        if hasattr(self.model, 'net') and hasattr(self.model.net, 'branch'):
            try:
                dummy_input_branch = torch.zeros(1, self.model.net.branch.linears[0].in_features)
                dummy_input_trunk = torch.zeros(1, self.model.net.trunk.linears[0].in_features)
                self.writer.add_graph(self.model, (dummy_input_branch, dummy_input_trunk))
            except Exception as e:
                print(f"è®°å½•æ¨¡å‹å›¾ç»“æ„æ—¶å‡ºé”™: {str(e)}")

    def on_epoch_end(self):
        """æ¯ä¸ªè®­ç»ƒæ­¥ç»“æŸæ—¶è°ƒç”¨"""
        # æ¯log_freqæ­¥è®°å½•ä¸€æ¬¡
        if self.step % self.log_freq == 0:
            try:
                loss_train = self.model.losshistory.loss_train[-1]
                self.writer.add_scalar('Loss/Train', loss_train, self.step)

                loss_test = self.model.losshistory.loss_test[-1]
                self.writer.add_scalar('Loss/Test', loss_test, self.step)

                # è®°å½•å„é¡¹æŒ‡æ ‡
                metrics_names = ["mean l2 relative error", "MAPE", "MAE", "RMSE"]
                for i, name in enumerate(metrics_names):
                    if i < len(self.model.losshistory.metrics_test[-1]):
                        metric_value = self.model.losshistory.metrics_test[-1][i]
                        self.writer.add_scalar(f"Scaled/{name}", metric_value, self.step)

                # === æ–°å¢éƒ¨åˆ†: åç¼©æ”¾åçš„æŒ‡æ ‡è®¡ç®— ===
                if self.scale_params is not None:
                    y_true_scaled = self.model.data.test_y
                    y_pred_scaled = self.model.predict(self.model.data.test_x)

                    sp = self.scale_params
                    if sp["type"] == "standard":
                        y_true = y_true_scaled * sp["y_std"] + sp["y_mean"]
                        y_pred = y_pred_scaled * sp["y_std"] + sp["y_mean"]
                    elif sp["type"] == "minmax":
                        y_true = y_true_scaled * (sp["y_max"] - sp["y_min"]) + sp["y_min"]
                        y_pred = y_pred_scaled * (sp["y_max"] - sp["y_min"]) + sp["y_min"]
                    else:
                        y_true, y_pred = y_true_scaled, y_pred_scaled

                    # è®¡ç®— RMSE / MAE / RÂ²
                    mse = np.mean((y_true - y_pred) ** 2)
                    rmse = np.sqrt(mse)
                    mae = np.mean(np.abs(y_true - y_pred))
                    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
                    ss_res = np.sum((y_true - y_pred) ** 2)
                    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
                    r2 = 1 - ss_res / (ss_tot + 1e-8)

                    # å†™å…¥ TensorBoard
                    self.writer.add_scalar("Unscaled/MAPE", mape, self.step)
                    self.writer.add_scalar("Unscaled/RMSE", rmse, self.step)
                    self.writer.add_scalar("Unscaled/MAE", mae, self.step)
                    self.writer.add_scalar("Unscaled/R2", r2, self.step)

                # --- ç½‘ç»œæƒé‡ï¼ˆbranch å’Œ trunkï¼‰ ---
                # if hasattr(self.model.net, "branch"):
                #     for i, layer in enumerate(self.model.net.branch.linears):
                #         self.writer.add_histogram(f"Branch/Layer{i}/weights", layer.weight, self.step)
                #         self.writer.add_histogram(f"Branch/Layer{i}/bias", layer.bias, self.step)
                #
                # if hasattr(self.model.net, "trunk"):
                #     for i, layer in enumerate(self.model.net.trunk.linears):
                #         self.writer.add_histogram(f"Trunk/Layer{i}/weights", layer.weight, self.step)
                #         self.writer.add_histogram(f"Trunk/Layer{i}/bias", layer.bias, self.step)
                # è®°å½•å­¦ä¹ ç‡
                lr = self.model.opt.param_groups[0]["lr"]
                self.writer.add_scalar("LearningRate", lr, self.step)

                # # ä¿å­˜æ£€æŸ¥ç‚¹
                # if self.ckpt_dir and self.step % self.save_freq == 0:
                #     save_checkpoint(
                #         self.model.net,
                #         self.model.opt,
                #         self.step,
                #         self.ckpt_dir
                #     )

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                # if loss_test < self.best_loss and self.ckpt_dir:
                #     self.best_loss = loss_test
                #     save_checkpoint(
                #         self.model.net,
                #         self.model.opt,
                #         self.step,
                #         self.ckpt_dir,
                #         is_best=True
                #     )

            except Exception as e:
                print(f"è®°å½•TensorBoardæ•°æ®æ—¶å‡ºé”™: {str(e)}")
                # å³ä½¿å‡ºé”™ä¹Ÿä¸ä¸­æ–­è®­ç»ƒ
        self.step += 1

    def on_train_end(self):
        """è®­ç»ƒç»“æŸæ—¶è°ƒç”¨"""
        print("è®­ç»ƒç»“æŸï¼Œå…³é—­TensorBoardå†™å…¥å™¨")
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        # if self.ckpt_dir:
        #     save_checkpoint(
        #         self.model.net,
        #         self.model.opt,
        #         self.step,
        #         self.ckpt_dir,
        #         is_best=False
        #     )
        self.writer.close()


# ===========================================================
# å‘½ä»¤è¡Œå‚æ•°è§£æ
# ===========================================================
def load_config(config_file):
    """ä»YAMLé…ç½®æ–‡ä»¶åŠ è½½å‚æ•°

    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        config: é…ç½®å­—å…¸
    """
    import yaml

    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    return config


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œä¼˜å…ˆä»é…ç½®æ–‡ä»¶å¯¼å…¥å‚æ•°é…ç½®

    Returns:
        args: è§£æåçš„å‚æ•°
    """
    # ç¬¬ä¸€æ­¥ï¼šå…ˆè§£æé…ç½®æ–‡ä»¶è·¯å¾„å‚æ•°
    config_parser = argparse.ArgumentParser(description="DeepONetæ¨¡å‹è®­ç»ƒå‚æ•°", add_help=False)
    config_parser.add_argument("--config", type=str, default="config.yaml", help="YAMLé…ç½®æ–‡ä»¶è·¯å¾„")
    config_args, _ = config_parser.parse_known_args()

    # åˆå§‹åŒ–é»˜è®¤å‚æ•°
    default_args = {}

    # å¦‚æœé…ç½®æ–‡ä»¶å­˜åœ¨ï¼Œä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°
    if config_args.config and os.path.exists(config_args.config):
        config = load_config(config_args.config)
        print("~~~~~~~~~~~~~~~~~~~~ä»é…ç½®æ–‡ä»¶åŠ è½½äº†å‚æ•°~~~~~~~~~~~~~~~~~~~~~~~~")

        # ä»é…ç½®æ–‡ä»¶æå–å‚æ•°
        if 'experiment' in config:
            exp_config = config['experiment']
            default_args['base_dir'] = exp_config.get('base_dir', 'runs')
            default_args['exp_name'] = exp_config.get('exp_name', None)

        if 'data' in config:
            data_config = config['data']
            default_args['branch_file'] = data_config.get('branch_file', 'branch_net_K_sampled.csv')
            default_args['trunk_file'] = data_config.get('trunk_file', 'trunk_net_K_sampled.csv')
            default_args['temp_file'] = data_config.get('temp_file', 'merged_all_time_points_K_sampled.csv')
            default_args['isPCA'] = data_config.get('isPCA', False)
            default_args['pca_dim'] = data_config.get('pca_dim', 512)
            default_args['test_size'] = data_config.get('test_size', 0.2)
            default_args['random_state'] = data_config.get('random_state', 42)
            default_args['scaling_method'] = data_config.get('scaling_method', "standard")

        if 'model' in config:
            model_config = config['model']
            default_args['branch_layers'] = model_config.get('branch_layers', [256, 256, 128])
            default_args['trunk_layers'] = model_config.get('trunk_layers', [64, 64, 128])
            default_args['activation'] = model_config.get('activation', 'tanh')
            default_args['initializer'] = model_config.get('initializer', 'Glorot normal')
            default_args['is_output_activation'] = model_config.get('is_output_activation', False)
            default_args['output_activation'] = model_config.get('output_activation', 'relu')
            default_args['isDropout'] = model_config.get('isDropout', False)
            default_args['dropout_rate'] = model_config.get('dropout_rate', 0.1)
            default_args['is_bias'] = model_config.get('is_bias', True)


        if 'training' in config:
            training_config = config['training']
            default_args['optimizer'] = training_config.get('optimizer', 'adamw')
            default_args['lr'] = training_config.get('lr', 0.0001)
            default_args['weight_decay'] = training_config.get('weight_decay', 1e-3)
            default_args['decay'] = training_config.get('decay', None)
            default_args['epochs'] = training_config.get('epochs', 10000)
            default_args['batch_size'] = training_config.get('batch_size', None)
            default_args['display_every'] = training_config.get('display_every', 1)

        if 'callbacks' in config:
            callbacks_config = config['callbacks']
            default_args['log_freq'] = callbacks_config.get('log_freq', 1)
            default_args['save_freq'] = callbacks_config.get('save_freq', 1000)
            default_args['metrics'] = callbacks_config.get('metrics', ["mean l2 relative error", "MAPE", "MAE", "RMSE"])

        if 'early_stopping' in config:
            early_stopping_config = config['early_stopping']
            default_args['isES'] = early_stopping_config.get('isES', False)
            default_args['monitor'] = early_stopping_config.get('monitor', 'loss_test')
            default_args['min_delta'] = early_stopping_config.get('min_delta', 1e-5)
            default_args['patience'] = early_stopping_config.get('patience', 3000)
            default_args['baseline'] = early_stopping_config.get('baseline', 1e-5)
            default_args['start_from_epoch'] = early_stopping_config.get('start_from_epoch', 1000)
    else:
        print(f"è­¦å‘Š: é…ç½®æ–‡ä»¶ {config_args.config} ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°")
        # è®¾ç½®é»˜è®¤å€¼
        default_args = {
            'base_dir': 'runs',
            'exp_name': None,
            'branch_file': 'branch_net_K_sampled.csv',
            'trunk_file': 'trunk_net_K_sampled.csv',
            'temp_file': 'merged_all_time_points_K_sampled.csv',
            'isPCA': False,
            'pca_dim': 512,
            'test_size': 0.2,
            'random_state': 42,
            'scaling_method': "standard",
            'branch_layers': [256, 256, 128],
            'trunk_layers': [64, 64, 128],
            'activation': 'tanh',
            'initializer': 'Glorot normal',
            'is_output_activation': False,
            'output_activation': 'relu',
            'isDropout': False,
            'dropout_rate': 0.1,
            'is_bias': True,
            'optimizer': 'adamw',
            'lr': 0.0001,
            'weight_decay': 1e-3,
            'decay': None,
            'epochs': 10000,
            'batch_size': None,
            'display_every': 1,
            'log_freq': 1,
            'save_freq': 1000,
            'metrics': ["mean l2 relative error", "MAPE", "MAE", "RMSE"],
            'isES': False,
            'monitor': 'loss_test',
            'min_delta': 1e-5,
            'patience': 3000,
            'baseline': 1e-5,
            'start_from_epoch': 1000
        }

    # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°ä½œä¸ºé»˜è®¤å€¼ï¼Œåˆ›å»ºä¸»å‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="DeepONetæ¨¡å‹è®­ç»ƒå‚æ•°")

    # é…ç½®æ–‡ä»¶
    parser.add_argument("--config", type=str, default=config_args.config, help="YAMLé…ç½®æ–‡ä»¶è·¯å¾„")

    # å®éªŒé…ç½®
    parser.add_argument("--base_dir", type=str, default=default_args['base_dir'], help="å®éªŒç»“æœä¿å­˜çš„åŸºç¡€ç›®å½•")
    parser.add_argument("--exp_name", type=str, default=default_args['exp_name'], help="å®éªŒåç§°ï¼Œé»˜è®¤ä½¿ç”¨æ—¶é—´æˆ³")

    # æ•°æ®é…ç½®
    parser.add_argument("--branch_file", type=str, default=default_args['branch_file'], help="åˆ†æ”¯ç½‘ç»œè¾“å…¥æ–‡ä»¶")
    parser.add_argument("--trunk_file", type=str, default=default_args['trunk_file'], help="ä¸»å¹²ç½‘ç»œè¾“å…¥æ–‡ä»¶")
    parser.add_argument("--temp_file", type=str, default=default_args['temp_file'], help="æ¸©åº¦æ ‡ç­¾æ–‡ä»¶")
    parser.add_argument("--isPCA", type=bool, default=default_args['isPCA'], help="æ˜¯å¦ä½¿ç”¨PCAé™ç»´")
    parser.add_argument("--pca_dim", type=int, default=default_args['pca_dim'], help="PCAé™ç»´ç»´åº¦")
    parser.add_argument("--test_size", type=float, default=default_args['test_size'], help="æµ‹è¯•é›†æ¯”ä¾‹")
    parser.add_argument("--random_state", type=int, default=default_args['random_state'], help="éšæœºç§å­")
    parser.add_argument("--scaling_method", type=str, default=default_args['scaling_method'], help="æ•°æ®é¢„å¤„ç†æ–¹æ³•")

    # æ¨¡å‹é…ç½®
    parser.add_argument("--branch_layers", type=int, nargs="+", default=default_args['branch_layers'],
                        help="åˆ†æ”¯ç½‘ç»œéšè—å±‚ç»“æ„")
    parser.add_argument("--trunk_layers", type=int, nargs="+", default=default_args['trunk_layers'],
                        help="ä¸»å¹²ç½‘ç»œéšè—å±‚ç»“æ„")
    parser.add_argument("--activation", type=str, default=default_args['activation'], help="æ¿€æ´»å‡½æ•°")
    parser.add_argument("--initializer", type=str, default=default_args['initializer'], help="åˆå§‹åŒ–æ–¹æ³•")

    parser.add_argument("--is_output_activation", type=str, default=default_args['is_output_activation'],
                        help="æ˜¯å¦è®¾ç½®è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°")
    parser.add_argument("--output_activation", type=str, default=default_args['output_activation'],
                        help="è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°")
    parser.add_argument("--isDropout", type=bool, default=default_args['isDropout'], help="æ˜¯å¦dropout")
    parser.add_argument("--dropout_rate", type=float, default=default_args['dropout_rate'], help="dropoutç‡")
    parser.add_argument("--is_bias", type=bool, default=default_args['is_bias'], help="è¾“å‡ºå±‚æ˜¯å¦è®¾ç½®åç½®")

    # è®­ç»ƒé…ç½®
    parser.add_argument("--optimizer", type=str, default=default_args['optimizer'], help="ä¼˜åŒ–å™¨ç±»å‹")
    parser.add_argument("--lr", type=float, default=default_args['lr'], help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=default_args['weight_decay'], help="æƒé‡è¡°å‡")
    parser.add_argument("--decay", type=str, nargs="+", default=default_args['decay'],
                        help="å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼Œä¾‹å¦‚: exponential 0.995 æˆ– cosine 10000 1e-7")
    parser.add_argument("--epochs", type=int, default=default_args['epochs'], help="è®­ç»ƒè½®æ¬¡")
    parser.add_argument("--batch_size", type=int, default=default_args['batch_size'], help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--display_every", type=int, default=default_args['display_every'], help="æ˜¾ç¤ºé¢‘ç‡")

    # å›è°ƒé…ç½®
    parser.add_argument("--log_freq", type=int, default=default_args['log_freq'], help="æ—¥å¿—è®°å½•é¢‘ç‡")
    parser.add_argument("--save_freq", type=int, default=default_args['save_freq'], help="æ¨¡å‹ä¿å­˜é¢‘ç‡")
    parser.add_argument("--metrics", type=str, nargs="+", default=default_args['metrics'], help="è¯„ä¼°æŒ‡æ ‡")

    # æ—©åœé…ç½®
    parser.add_argument("--isES", type=str, default=default_args['isES'], help="æ˜¯å¦è®¾ç½®æ—©åœ")
    parser.add_argument("--monitor", type=str, default=default_args['monitor'], help="æ—©åœç›‘æ§æŒ‡æ ‡")
    parser.add_argument("--min_delta", type=float, default=default_args['min_delta'], help="æœ€å°æ”¹å–„å¹…åº¦")
    parser.add_argument("--patience", type=int, default=default_args['patience'], help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--baseline", type=float, default=default_args['baseline'], help="æ—©åœåŸºå‡†å€¼")
    parser.add_argument("--start_from_epoch", type=int, default=default_args['start_from_epoch'], help="æ—©åœå¼€å§‹è½®æ¬¡")

    args = parser.parse_args()

    # å¤„ç†å­¦ä¹ ç‡è¡°å‡å‚æ•°
    if args.decay is not None:
        if isinstance(args.decay, list) and len(args.decay) > 0:
            if args.decay[0] == "exponential":
                args.decay = ("exponential", float(args.decay[1]))
            elif args.decay[0] == "cosine":
                args.decay = ("cosine", int(args.decay[1]), float(args.decay[2]))
            elif args.decay[0] == "step":
                args.decay = ("step", int(args.decay[1]), float(args.decay[2]))
            elif args.decay[0] == "inverse time":
                args.decay = ("step", int(args.decay[1]), float(args.decay[2]))
            # elif args.decay[0] == "lambda": # ("lambda", lambda_fn: Callable[[step], float])
            #     args.decay = ("lambda", args.decay[1])

    print("*" * 100)
    print("æœ€ç»ˆå‚æ•°é…ç½®:")
    for key, value in vars(args).items():
        print(f"  {key}: {value}")
    print("*" * 100)

    return args


def load_data(branch_file, trunk_file, temp_file):
    """åŠ è½½æ•°æ®æ–‡ä»¶

    Args:
        branch_file: åˆ†æ”¯ç½‘ç»œè¾“å…¥æ–‡ä»¶è·¯å¾„
        trunk_file: ä¸»å¹²ç½‘ç»œè¾“å…¥æ–‡ä»¶è·¯å¾„
        temp_file: æ¸©åº¦æ ‡ç­¾æ–‡ä»¶è·¯å¾„

    Returns:
        branch_df: åˆ†æ”¯ç½‘ç»œè¾“å…¥æ•°æ®
        trunk_df: ä¸»å¹²ç½‘ç»œè¾“å…¥æ•°æ®
        temp_df: æ¸©åº¦æ•°æ®
        temp_df_with_titles: å¸¦æ ‡é¢˜çš„æ¸©åº¦æ•°æ®
    """
    # trunk_net.csv: æ¯è¡Œä¸€ä¸ª node_id (index)ï¼Œåˆ—ä¸º x,y,z
    trunk_df = pd.read_csv(trunk_file, index_col=0)
    print(f"[INFO] trunk_net ç»´åº¦: {trunk_df.shape} (nodes Ã— xyz)")

    # merged_all_time_points.csv: æ¯ä¸ªæ ·æœ¬çœŸå®æ¸©åº¦ (test, step, increment, node_1...node_N)
    temp_df = pd.read_csv(temp_file, index_col=0)
    temp_df_with_titles = temp_df.copy()  # ä¿ç•™å®Œæ•´åˆ—åçš„å‰¯æœ¬
    temp_df.drop(columns=['test', 'step', 'increment'], inplace=True)
    print(f"[INFO] æ¸©åº¦æ•°æ® {temp_file} ç»´åº¦: {temp_df.shape}")

    # branch_net.csv: æ¯è¡Œä¸€ä¸ª sampleï¼Œæ¯åˆ—ä¸€ä¸ª node (åˆ—åä¸º node_id)
    branch_df = pd.read_csv(branch_file, index_col=0)
    branch_df = branch_df.iloc[temp_df.index]
    print(f"[INFO] branch_net ç»´åº¦: {branch_df.shape} (samples Ã— nodes)")

    return branch_df, trunk_df, temp_df, temp_df_with_titles


def prepare_data(branch_df, trunk_df, temp_df, isPCA=False, pca_dim=512, test_size=0.2, random_state=42,
                 scaling_method="standard"):
    """
    å‡†å¤‡ DeepONet è¾“å…¥è¾“å‡ºæ•°æ®

    Args:
        branch_df: åˆ†æ”¯ç½‘ç»œè¾“å…¥æ•°æ®
        trunk_df: ä¸»å¹²ç½‘ç»œè¾“å…¥æ•°æ®
        temp_df: æ¸©åº¦æ•°æ®
        isPCA: æ˜¯å¦ä½¿ç”¨ PCA é™ç»´
        pca_dim: PCA é™ç»´ç»´åº¦
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        random_state: éšæœºç§å­
        scaling_method: æ•°æ®ç¼©æ”¾æ–¹å¼ ("standard" | "minmax" | "none")

    Returns:
        data: DeepONet æ•°æ®ç»“æ„
        X_branch: åŸå§‹åˆ†æ”¯è¾“å…¥
        scale_params: åŒ…å«ç”¨äºé€†å˜æ¢çš„å‚æ•°å­—å…¸
        X_branch_test_scaled: å¤„ç†åçš„æµ‹è¯•é›†åˆ†æ”¯è¾“å…¥
        X_trunk_test: æµ‹è¯•é›†ä¸»å¹²è¾“å…¥
        y_test_scaled: å¤„ç†åçš„æµ‹è¯•é›†è¾“å‡º
    """
    # Branch è¾“å…¥ï¼šçƒ­æµå¯†åº¦
    X_branch = branch_df.to_numpy(dtype=np.float32)
    if isPCA:
        pca = PCA(n_components=pca_dim)
        X_branch = pca.fit_transform(X_branch)
        print(f"[INFO] PCA åç»´åº¦: {X_branch.shape}")

    # Trunk è¾“å…¥ï¼šèŠ‚ç‚¹åæ ‡
    X_trunk = trunk_df.to_numpy(dtype=np.float32)

    # è¾“å‡º yï¼šæ¸©åº¦
    y = temp_df.to_numpy(dtype=np.float32)

    # åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
    X_branch_train, X_branch_test, y_train, y_test = train_test_split(
        X_branch, y, test_size=test_size, random_state=random_state
    )
    X_trunk_train = X_trunk
    X_trunk_test = X_trunk

    print(f"[INFO] è®­ç»ƒæ ·æœ¬æ•°: {X_branch_train.shape[0]}, æµ‹è¯•æ ·æœ¬æ•°: {X_branch_test.shape[0]}")
    print(f"[INFO] èŠ‚ç‚¹æ•°: {X_trunk.shape[0]}")

    # æ ¹æ® scaling_method é€‰æ‹©ä¸åŒçš„æ•°æ®é¢„å¤„ç†æ–¹å¼
    if scaling_method == "standard":
        # === æ ‡å‡†åŒ– ===
        branch_mean = X_branch_train.mean(axis=0, keepdims=True)
        branch_std = X_branch_train.std(axis=0, keepdims=True) + 1e-8
        X_branch_train_scaled = (X_branch_train - branch_mean) / branch_std
        X_branch_test_scaled = (X_branch_test - branch_mean) / branch_std

        # y_mean = np.mean(y_train)
        # y_std = np.std(y_train) + 1e-8
        # y_train_scaled = (y_train - y_mean) / y_std
        # y_test_scaled = (y_test - y_mean) / y_std

        y_mean = np.mean(y_train, axis=0, keepdims=True)
        y_std = np.std(y_train, axis=0, keepdims=True) + 1e-8
        y_train_scaled = (y_train - y_mean) / y_std
        y_test_scaled = (y_test - y_mean) / y_std

        scale_params = {
            "type": "standard",
            "branch_mean": branch_mean,
            "branch_std": branch_std,
            "y_mean": y_mean,
            "y_std": y_std,
        }

    elif scaling_method == "minmax":
        # === å½’ä¸€åŒ– ===
        branch_min = X_branch_train.min(axis=0, keepdims=True)
        branch_max = X_branch_train.max(axis=0, keepdims=True)
        X_branch_train_scaled = (X_branch_train - branch_min) / (branch_max - branch_min + 1e-8)
        X_branch_test_scaled = (X_branch_test - branch_min) / (branch_max - branch_min + 1e-8)

        y_min = np.min(y_train)
        y_max = np.max(y_train)
        y_train_scaled = (y_train - y_min) / (y_max - y_min + 1e-8)
        y_test_scaled = (y_test - y_min) / (y_max - y_min + 1e-8)

        scale_params = {
            "type": "minmax",
            "branch_min": branch_min,
            "branch_max": branch_max,
            "y_min": y_min,
            "y_max": y_max,
        }

    else:
        # === ä¸åšä»»ä½•å¤„ç† ===
        X_branch_train_scaled = X_branch_train
        X_branch_test_scaled = X_branch_test
        y_train_scaled = y_train
        y_test_scaled = y_test

        scale_params = {"type": "none"}

    # æ„é€  DeepONet æ•°æ®ç»“æ„
    data = dde.data.TripleCartesianProd(
        X_train=(X_branch_train_scaled, X_trunk_train),
        y_train=y_train_scaled,
        X_test=(X_branch_test_scaled, X_trunk_test),
        y_test=y_test_scaled,
    )

    return data, X_branch, scale_params, X_branch_test_scaled, X_trunk_test, y_test


class CustomDeepONet(dde.nn.DeepONetCartesianProd):
    def __init__(
            self,
            layer_sizes_branch,
            layer_sizes_trunk,
            activation,
            kernel_initializer,
            is_output_activation=False,
            output_activation="relu",  # æ–°å¢å‚æ•°ï¼šè¾“å‡ºå±‚æ¿€æ´»å‡½æ•°
            num_outputs=1,
            multi_output_strategy=None,
            isDropout=False,
            dropout_rate=0.1,
            is_bias=True,
            init_bias=None
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            layer_sizes_branch,
            layer_sizes_trunk,
            activation,
            kernel_initializer,
            num_outputs,
            multi_output_strategy
        )

        # ä¿å­˜è¾“å‡ºæ¿€æ´»å‡½æ•°
        self.output_activation_name = output_activation
        self.output_activation = self._get_output_activation(output_activation)

        # ä¸ºbranchå’Œtrunkç½‘ç»œæ·»åŠ dropoutå±‚
        self.isDropout = isDropout
        self.is_output_activation = is_output_activation
        self.branch_dropout = nn.Dropout(dropout_rate)
        self.trunk_dropout = nn.Dropout(dropout_rate)

        self.is_bias = is_bias
        # å¯å­¦ä¹ åç½®ï¼Œç»´åº¦ä¸ºè¾“å‡ºç»´åº¦ï¼ˆé€šå¸¸ç­‰äºèŠ‚ç‚¹æ•°çš„ä¸€éƒ¨åˆ†ï¼‰ï¼›å¦‚æœnum_outputs==1ä¹Ÿå¯ä»¥ç”¨æ ‡é‡
        if is_bias and init_bias is None:
            init_bias = 0.0
            # ä½¿ç”¨å‚æ•°å¼ é‡ï¼Œå…è®¸åå‘ä¼ æ’­å­¦ä¹ 
        self.output_bias = nn.Parameter(torch.tensor(init_bias, dtype=torch.float32))

    def _get_output_activation(self, activation_name):
        """è·å–è¾“å‡ºæ¿€æ´»å‡½æ•°"""
        activations = {
            "relu": nn.ReLU(),
            "softplus": nn.Softplus(),
            "sigmoid": nn.Sigmoid(),
            "tanh": nn.Tanh(),
            "elu": nn.ELU(),
            "gelu": nn.GELU(),
            "linear": lambda x: x  # æ’ç­‰æ˜ å°„ï¼Œå³æ— æ¿€æ´»
        }
        return activations.get(activation_name.lower(), nn.ReLU())

    def merge_branch_trunk(self, x_func, x_loc, index):
        """é‡å†™åˆå¹¶æ–¹æ³•ï¼Œåœ¨è¾“å‡ºå‰åº”ç”¨æ¿€æ´»å‡½æ•°"""
        # è°ƒç”¨çˆ¶ç±»çš„è®¡ç®—
        y = super().merge_branch_trunk(x_func, x_loc, index)
        if self.is_output_activation:
            # åº”ç”¨è¾“å‡ºæ¿€æ´»å‡½æ•°
            y = self.output_activation(y)
        if self.is_bias:
            y=y + self.output_bias
        return y

    def forward(self, inputs):
        x_func = inputs[0]
        x_loc = inputs[1]

        # Trunk net input transform
        if self._input_transform is not None:
            x_loc = self._input_transform(x_loc)

        if self.isDropout:
            # åº”ç”¨dropout
            x_func = self.branch_dropout(x_func)
            x_loc = self.trunk_dropout(x_loc)

        x = self.multi_output_strategy.call(x_func, x_loc)

        if self._output_transform is not None:
            x = self._output_transform(inputs, x)
        return x


# ===========================================================
# æ¨¡å‹å®šä¹‰æ¨¡å—
# ===========================================================
def create_model(data, branch_dim, trunk_dim=3, branch_layers=[256, 256, 128],
                 trunk_layers=[64, 64, 128], activation="tanh", initializer="Glorot normal", is_output_activation=False,
                 output_activation="softplus",isDropout=False, dropout_rate=0.1,is_bias=True,init_bias=None):
    """åˆ›å»ºDeepONetæ¨¡å‹

    Args:
        data: DeepONetæ•°æ®ç»“æ„
        branch_dim: åˆ†æ”¯ç½‘ç»œè¾“å…¥ç»´åº¦
        trunk_dim: ä¸»å¹²ç½‘ç»œè¾“å…¥ç»´åº¦
        branch_layers: åˆ†æ”¯ç½‘ç»œéšè—å±‚ç»“æ„
        trunk_layers: ä¸»å¹²ç½‘ç»œéšè—å±‚ç»“æ„
        activation: æ¿€æ´»å‡½æ•°
        initializer: åˆå§‹åŒ–æ–¹æ³•

    Returns:
        model: DeepONetæ¨¡å‹
    """
    # æ„å»ºå®Œæ•´çš„ç½‘ç»œç»“æ„
    branch_net = [branch_dim] + branch_layers
    trunk_net = [trunk_dim] + trunk_layers

    # åˆ›å»ºDeepONetç½‘ç»œ
    # net = dde.nn.DeepONetCartesianProd(
    #     branch_net,  # branch net
    #     trunk_net,  # trunk net
    #     activation,
    #     initializer,
    # )

    net = CustomDeepONet(
        branch_net,
        trunk_net,
        activation,
        initializer,
        is_output_activation=is_output_activation,
        output_activation=output_activation,  # æ¨èä½¿ç”¨softplusï¼Œæ¯”ReLUæ›´å¹³æ»‘
        isDropout=isDropout,
        dropout_rate=dropout_rate,
        is_bias=is_bias,
        init_bias=init_bias
    )
    # åˆ›å»ºæ¨¡å‹
    model = dde.Model(data, net)

    return model


# ===========================================================
# ç»“æœå¯è§†åŒ–å’Œè¯„ä¼°æ¨¡å—
# ===========================================================
def plot_loss_history(loss_history, model, fname=None):
    """ç»˜åˆ¶æŸå¤±æ›²çº¿

    Args:
        loss_history: æŸå¤±å†å²
        model: æ¨¡å‹
        fname: ä¿å­˜æ–‡ä»¶å
    """
    # å¤„ç†ä¸åŒé•¿åº¦çš„æ•°ç»„
    loss_train = np.array([np.sum(loss) for loss in loss_history.loss_train])
    loss_test = np.array([np.sum(loss) for loss in loss_history.loss_test])

    plt.figure(figsize=(10, 6))
    plt.semilogy(loss_history.steps, loss_train, label="Train loss")
    plt.semilogy(loss_history.steps, loss_test, label="Test loss")
    # ä½¿ç”¨å®é™…çš„metricsåç§°è€Œä¸æ˜¯å‡½æ•°å¼•ç”¨
    # metrics_names = ["mean l2 relative error", "MAPE", "MAE", "RMSE"]
    # for i in range(len(loss_history.metrics_test[0])):
    #     plt.semilogy(
    #         loss_history.steps,
    #         np.array(loss_history.metrics_test)[:, i],
    #         label=metrics_names[i],
    #     )
    plt.xlabel("# Steps")
    plt.ylabel("Loss/Metrics")
    plt.title("Training Progress")
    plt.legend()
    plt.grid(True, which="both", ls="--", alpha=0.3)

    if isinstance(fname, str):
        plt.savefig(fname, dpi=300)
        print(f"æŸå¤±æ›²çº¿å·²ä¿å­˜åˆ°: {fname}")


def evaluate_model(y_true, y_pred, output_dir):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½

    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        output_dir: è¾“å‡ºç›®å½•
    """
    # è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
    mse = np.mean((y_true - y_pred) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(y_true - y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
    r2 = 1 - np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2)

    # æ‰“å°è¯„ä¼°ç»“æœ
    print("\n" + "=" * 50)
    print("æ¨¡å‹è¯„ä¼°ç»“æœ:")
    print(f"MSE: {mse:.6f}")
    print(f"RMSE: {rmse:.6f}")
    print(f"MAE: {mae:.6f}")
    print(f"MAPE: {mape:.2f}%")
    print(f"RÂ²: {r2:.6f}")
    print("=" * 50)

    # ä¿å­˜è¯„ä¼°ç»“æœ
    metrics = {
        "MSE": float(mse),
        "RMSE": float(rmse),
        "MAE": float(mae),
        "MAPE": float(mape),
        "R2": float(r2)
    }

    metrics_path = os.path.join(output_dir, "evaluation_metrics.json")
    with open(metrics_path, "w") as f:
        json.dump(metrics, f, indent=4)
    print(f"è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_path}")


# ===========================================================
# ä¸»å‡½æ•°
# ===========================================================
if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()

    # åˆ›å»ºå®éªŒç›®å½•
    exp_dir, log_dir, ckpt_dir, output_dir = create_experiment_dir(
        base_dir=args.base_dir,
        exp_name=args.exp_name
    )

    # åˆ›å»ºSummaryWriterå®ä¾‹
    writer = SummaryWriter(log_dir=log_dir)

    # ä¿å­˜é…ç½®
    config = vars(args)
    save_config(config, exp_dir)

    # å‡†å¤‡æ•°æ®
    # data, X_branch, scale_params, X_branch_test_scaled, X_trunk_test, y_test

    # è¿è¡ŒOptunaä¼˜åŒ–
    study = run_optuna_optimization(
        args, output_dir
    )

    # åˆ†æå’Œä¿å­˜ç»“æœ
    best_params = analyze_optuna_results(study, output_dir)

    print("\nâœ… Optunaä¼˜åŒ–å®Œæˆï¼")
    print("ä½¿ç”¨æœ€ä½³è¶…å‚æ•°é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")

